<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Presentation</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">

		<style type="text/css">
			:root {
				--r-main-font-size: 22pt;
			}
		</style>
		
		<!-- Font awesome is required for the chalkboard plugin -->
		<script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/js/all.min.js"></script>
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
		<!-- Custom controls plugin is used to for opening and closing annotation modes. -->
		<script src="https://cdn.jsdelivr.net/npm/reveal.js-plugins@latest/customcontrols/plugin.js"></script>
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js-plugins@latest/customcontrols/style.css">
		<!-- Chalkboard plugin -->
		<script src="https://cdn.jsdelivr.net/npm/reveal.js-plugins@latest/chalkboard/plugin.js"></script>
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js-plugins@latest/chalkboard/style.css">



	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section>
					<section data-menu-title="Title" data-autoplay="true" data-background-video="videos/drone.mp4" data-background-video-loop="loop" data-background-video-muted="true">
						<h2>Text Mining in Accounting</h2>
						<h4>NSYSU Fall 2024
						</h4>
						<br>
						<br>
						<p>Dr. Wil Martens</p>
					</section>

				</section>

				<section>
					<h5 style="text-align: right;">Introduction</h5>

					<section data-menu-title="Introduction" data-autoplay="true" data-background-video="videos/sunrise.mp4" data-background-video-loop="loop" data-background-video-muted="true">
						
					</section>

					<section>
						<h2>Text mining has many names </h2>
						<br>
						<ul>
							<li>Knowledge discovery from text (KDT)</li>
							<li>Intelligent Text Analysis</li>
							<li>Text Data Mining</li>
							<li>Natural Language Processing</li>
						</ul>
					</section>

					<section>
						<h2>"AI"</h2>
						<img src="images/ai_venn.png" width="85%">
					</section>

					<section>
						<h2>What it entails</h2>
						<img src="images/mining_process.png" width="85%">
					</section>		

					<section>
						<h2>Outline</h2>
						<br>
						<ol>
							<li>Preprocessing</li>
							<ul>
								<li>Tokenization</li>
								<li>Filtering</li>
								<li>Lemmatization or stemming</li>
							</ul>
							<li>Transformation approaches</li>
							<ul>
								<li>Bag of words </li>
								<li>Approaches</li>
							</ul>
							<li>Analysis techniques</li>
							<ul>
								<li>Classification</li>
								<li>Clustering</li>
								<li>Information extraction</li>
								<li>Text evaluation</li>
							</ul>
							<li>Applications</li>
						</ol>
					</section>
				</section>

				<section>
					<h5 style="text-align: right;">Preprocessing</h5>

					<section data-menu-title="Preprocessing" data-autoplay="true" data-background-video="videos/cooking.mp4" data-background-video-loop="loop" data-background-video-muted="true">
						
					</section>

					<section>
						<h2>Tokenization</h2>
						<br>
						<ul>
							<li>Segment a text into units or "tokens" of pre-determined length</li>
							<ul>
								<li>Word pieces</li>
								<li>Sentences</li>
								<li>Paragraphs</li>
							</ul>
							<li>Split based on delimiters</li>
								<ul>
									<li>Spaces</li>
									<li>Tabs</li>
									<li>Conjunctions</li>
									<li>Specific preselected words</li>
								</ul>
							<li>The set of identified tokens is called a dictionary</li>
						</ul>
					</section>

					<section>
						<h2>Filtering</h2> 
						<br>
						<ul>
							<li>Removal of stop words</li>
							<img src="images/stop-words.png" width="85%">
							<li>Can remove noise from data</li>
						</ul>
					</section>

					<section>
						<h2>Stemming</h2> 
						<br>
						<ul>
							<li>Segment a text into units or "tokens" of pre-determined length
								<ul>
									<li>Convert variations of same word to word stem</li>
									<li>Coarse stemming methods remove prefixes and affixes</li>
									<img src="images/stemm.png" width="55%">
								</ul>
							</li>
					</section>

					<section>
						<h2>Lemmatization</h2> 
						<br>
						<ul>
							<li>Convert variations of words to "lemma"</li>
							<li>More complex, uses Part of Speech labels for each token 
								<ul>
									<li>nouns, verbs, adjectives, adverb, etc.</li>
								</ul>
							</li>
							<li>Verbs transformed into infinite tense, plural nouns reversed to singular</li>
							<img src="images/lemm.png" width="55%">
						</ul>
					</ul>
					</section>

				</section>

				<section>
					<h5 style="text-align: right;">Transformation Approaches</h5>

					<section data-menu-title="Transformation Approaches" data-autoplay="true" data-background-video="videos/data.mp4" data-background-video-loop="loop" data-background-video-muted="true">
					</section>

					<section>
						<h2>Bag of Words</h2> 
						<br>
						<ul>
							<li>Change text into a list of word counts</li>
							<li>Token frequencies can be normalized to account for document length</li>
							<li>Produces a "Term Document Frequency Matrix"</li>
							<img src="images/frequency-matrix.png" width="55%">
							<li>Drawbacks:
								<ul>
									<li>You end up with a lot of columns (sparse matrix)</li>
									<li>Only takes token frequencies into account</li>
									<li>Ignores order, interdependencies, and context</li>
								</ul>
							</li>

							
						</ul>

					</section>

					<section>
						<h2>Vector Space Models</h2> 
						<br>
						<ul>
							<li>Represent a document as an n-dimensional vector of token weights</li>
							<li>Weight can be frequency, or binary value indicating if term is present</li>
							<li>For example: [0.1, -0.4, 0.5, 0.9, -0.2, … ]</li>
							<img src="images/vector-space.png" width="45%">
						</ul>

					</section>

				</section>

				<section>
					<h5 style="text-align: right;">Classification</h5>
					
					<section data-menu-title="Classification" data-autoplay="true" data-background-video="videos/surfing.mp4" data-background-video-loop="loop" data-background-video-muted="true"></section>

					<section>
						<h2>Classification</h2>
						<ul>
							<li>AKA 'categorization'</li>
							<li>Uses labeled data to train model</li>
							<li>The model learns to assign labels (classes) to new text fragments</li>
							<li>Subcategories:
								<ul>
									<li>Topic classification</li>
									<li>Sentiment analysis</li>
								</ul>
							</li>
						</ul>
					</section>

					<section>
						<h2>Naïve Bayes Classifier</h2>
						<img src="images/naive-bayes.png" width="75%" style="filter: contrast(80%);">
						<ul>
							<li>Use a set of labelled texts to train a classification model</li>
							<li>Determines probability of a label given a set of conditions</li>
							<li>Generally easy and quick but inaccurate</li>
							<li>Assumes probability of word being present is independent and equally important</li>
						</ul>

					</section>

					<section>
						<h2>Decision Trees</h2>
						<img src="images/decision-tree.png" width="55%" style="filter: contrast(80%);">
						<ul>
							<li>Uses a set of rules, and uses consecutive criteria to classify a new text</li>
							<li>Fast and scalable, better than Naïve Bayes classifier but still relatively imprecise</li>
							<li>A bunch of them together is called a "Random Forest"
								<ul>
									<li>Verdicts of many uncorrelated trees combined to choose best label</li>
								</ul>
							</li>
						
						</ul>

					</section>

					<section>
						<h2>Support Vector Machines</h2>
						<img src="images/support-vector.png" width="55%" style="filter: contrast(80%);">
						<ul>
							<li>Uses 'hyperplanes' in the space of document vectors as boundary between different labels</li>
							<li>Maximizes their distance (‘margin’) to the nearest instances of the labels they are supposed to separate</li>
							<li>New texts are classified based on their vector representations’ positions relative to the hyperplanes</li>
						</ul>
					</section>

					<section>
						<h2>Sentiment Analysis</h2>

						<ul>
							<li>Determines the overall sentiment value of a text </li>
							<li>Can be considered the 'tone' of a text </li>
							<li>Positive vs. negative (and sometimes neutral) </li>
							<li>Uses predetermined (general or context-specific) dictionaries or word lists that connect words to their tone
								<ul>
									<li>General Inquirer, Diction, or LIWC</li>
								</ul>
							</li>
						</ul>
						<img src="images/liwc.png" width="45%" style="filter: contrast(80%);">
					</section>

				</section>

				<section>
					<h5 style="text-align: right;">Clustering</h5>
					
					<section data-menu-title="Clustering" data-autoplay="true" data-background-video="videos/waves.mp4" data-background-video-loop="loop" data-background-video-muted="true"></section>


					<section>
						<h2>Clustering</h2>
						<img src="images/dendogram.png" width="55%" style="filter: contrast(80%);">
						<ul>
							<li>Aims to gather text collections whose members are highly similar to each other</li>
							<li>Categorizes text fragments without using predefined labels which allows new associations between documents to be uncovered
								<ul>
									<li>Similarity Measures</li>
									<li>K-means clustering</li>
									<li>Latent Dirichlet Allocation (LDA)</li>
								</ul>
							</li>
						</ul>
					</section>

					<section>
						<h2>Similarity Measures</h2>
						<img src="images/cosine-similarity.jpg" width="35%" style="filter: contrast(80%);">
						<li>Cosine similarity is the cosine of the angle between two document vector representations of text documents</li>
						<img src="images/jaccard-similarity.jpg" width="55%" style="filter: contrast(80%);">
						<li>Jaccard Similarity is a measure of similarity between two asymmetric binary vectors or we can say a way to find the similarity between two sets</li>
					</section>

					<section>
						<h2>K-means</h2>
						<img src="images/k-means.gif" width="35%" style="filter: contrast(80%);">
						<ul>	
							<li>Uses unlabeled, unclassified data</li>
							<li>Assigns data points to one K cluster using distance from the center of the cluster
							<li>Starts randomly assigning cluster centroids, each data point assigned to cluster, new cluster centroids are assigned, process runs iteratively</li>
						</ul>
					</section>

					<section>
						<h2>Latent Dirichlet Allocation</h2>
						<img src="images/lda.png" width="55%" style="filter: contrast(80%);">
						<ul>
							<li>Identifies latent structures in unstructured text</li>
							<li>You decide the number of topics to be identified and the number of words that can be used to characterize a topic</li>
							<li>The LDA algorithm looks at co-occurrences to identify the most prominent topics among the words</li>
							<li>Requires interpretation to derive a meaningful topic label</li>
						</ul>
					</section>

				</section>

				<section>
					<h5 style="text-align: right;">Information extraction & Text evaluation</h5>
					
					<section data-menu-title="Information extraction & Text evaluation" data-autoplay="true" data-background-video="videos/references.mp4" data-background-video-loop="loop" data-background-video-muted="true"></section>

					<section>
						<h2>Information extraction</h2>
						<ul>
							<li>When you provide a pre-defined list of words or expressions of interest</li>
							<li>Like a 'targeted search' for the predefined list and highly related words or phrases can be identified in a set of text corpora</li>
							<li>Terms related to certain types of risk
								<ul>
									<li>Litigation risk</li>
									<li>Going-concern risk</li>
									<li>Fraud risk</li>
								</ul>
							</li>
							<li>Targeted searches for 'entity recognition'
								<ul>
									<li>Highlight any references to a person, company or other entity under consideration
									</li>
								</ul>
							</li>
						</ul>
					</section>


					<section>
						<h2>Text evaluation</h2>
						<ul>
							<li>Texts can be evaluated on objective aspects like readability </li>
							<li>Readability measures:
								<ul>
									<li>Fog Index</li>
									<li>Disclosure quantity</li>
									<li>LM PE index</li>
									<li>Bog index</li>
								</ul>
							</li>
							<li>A readability index is designed to measure the effort required for a reader to process a text and understand its intended message</li>
							<li>Accounting reports that are difficult to interpret could indicate intentional information obfuscation by a company’s management and low user-friendliness</li>
						</ul>
					</section>

					<section>
						<h2>Readability Measures</h2>
						<ul>
							<li>Fog Index
								<ul>
									<li>Uses the SEC plain English guidelines checking for ‘plain English violations’</li>
									<li>Long sentences and words counting three or more syllables are considered to be indicators of low text readability</li>
								</ul>
							</li>
							<li>Disclosure quantity 
								<ul>
									<li>Looks at the number of words in a text</li>
									<li>Also sometimes measured using the size of a file containing a text</li>
								</ul>
							</li>
						</ul>
					</section>


					<section>
						<h2>Readability Measures</h2>
						<ul>
							<li>LM PE index 
								<ul>
									<li>Also uses the SEC plain English guidelines checking for ‘plain English violations’</li>
									<li>Long sentences, complex words, passive verb forms, and legal terms</li>
								</ul>
							</li>
							<li>Bog Index 
								<ul>
									<li>Covers all SEC plain English guidelines, such as the avoidance of passive voice, superfluous words, unnecessary detail and complex words</li>
									<li>Word complexity determined using a list of over 200,000 words</li>
									<li>Rated on familiarity (i.e. obscurity, technicality) and precision</li>
									<li>Takes into consideration words and constructs that could deliberately increase readability</li>
									<li>May be more of a measure of writing style, rather than a measure of readability</li>
								</ul>
							</li>
						</ul>
					</section>

				</section>

				<section>
					<h5 style="text-align: right;">Applications</h5>
					
					<section data-menu-title="Applications" data-autoplay="true" data-background-video="videos/magnify.mp4" data-background-video-loop="loop" data-background-video-muted="true"></section>
					
					<section>
						<h2>Accounting Applications</h2>
						<ul>
							<li>Risk and going-concern assessment</li>
							<li>Fraud detection</li>
							<li>Increase of user-friendliness accounting documentation</li>
							<li>Prediction of future performance</li>
						</ul>
					</section>

					<section>
						<h2>Risk and going-concern assessment</h2>
						<ul>
							<li>Depending on the wording they choose, a company’s management could inadvertently reveal the company’s risk profile</li>
							<li>Calculating the number of appearances of words expressing risk or uncertainty (e.g. ‘risk’, ‘risky’, ‘uncertain’) could assesses anticipated risk</li>
							<li>Certain words and expressions in an annual report could indicate whether a company is at risk of going bankrupt
								<ul>
									<li>‘to our regret’ or ‘does not allow’ appearing in the same section as ‘dividend’ could indicate high probability of failure </li>
									<li>Frequent co-occurrences of expressions such as ‘research and development’ and ‘dividend’ are indicative of corporate continuity</li>
								</ul>
							</li>
						</ul>
					</section>

					<section>
						<h2>Fraud detection</h2>
						<ul>
							<li>Managers often unconsciously reveal information in reports through language commonly associated with obfuscation and impression management 
								<ul>
									<li>Mechanism could be seen as an attempt to distract the reader from the financial results or the ploys used to stage the results</li>
								</ul>
							</li>

							<li>Mechanism could be seen as an attempt to distract the reader from the financial results or the ploys used to stage the results
								<ul>
									<li>Higher, more persistent earnings positively correlated with the readability of the financial reports</li>
								</ul>
							</li>
						<li>Managers that frequently obscure their writing predictive of poor performance or earnings</li>
						</ul>
					</section>

					<section>
						<h2>Fraud detection</h2>
						<ul>
							<li>Examining content-related features of annual reports can be applied to fraud detection</li>

							<li>You can identify early and advanced stages of fraud using certain simple surface, or deeper linguistic features

								<ul>
									<li>Percentage of sentences in active vs. passive voice </li>
									<li>Readability index </li>
									<li>Standard deviation of sentence length </li>
								</ul>
							</li>

						<li>Perhaps fraudulent managers tend to write more often in passive voice, in order to dissociate themselves from the ongoing situation</li>
						</ul>
					</section>

					<section>
						<h2>Fraud detection</h2>
						<ul>
							<li>Cosine similarity metric can be used to define ‘abnormal disclosure’ as the deviation between the MD&A word distribution vector and the average word usage vector of non-fraudulent industry peers</li>

							<li>You can track the evolution of similarity  of a company’s year-to-year annual reports


								<ul>
									<li>Can expose an incongruity in its reporting activities in a particular period of time</li>
									<li>Incongruity might in turn be attributable to fraudulent activities </li>
								</ul>
							</li>

						<li>A ‘Fraud Similarity Score’ can uncover fraud cases by calculating the cosine similarity between a firm’s abnormal disclosure and the average abnormal disclosure vector of firms previously involved in SEC AAER enforcement actions
						</li>
						</ul>
					</section>

					<section>
						<h2>Increase user-friendliness</h2>
						<ul>
							<li>Accounting documentation and annual reports include additional disclosures that are becoming progressively more extensive and complex</li>

							<li>User-friendliness and accessibility of the information in accounting text documents could equally be increased through certain textual aspects



								<ul>
									<li>Summarization</li>
									<li>Topic detection</li>
									<li>Automatic information extraction</li>
									<li>Integrate complementary qualitative and quantitative information</li>
								</ul>
							</li>

						<li>Quality of corporate communication impacts on the speed and effectiveness of investors’ decision-making so it may improve future stock returns and earnings</li>
						</ul>
					</section>

					<section>
						<h2>Prediction of performance</h2>
						<ul>
							<li>Positive, negative or neutral tone of management communications can reveal management’s predictions of future performance</li>

							<li>Positive word connotations in earnings press releases and earnings conference calls has been linked to:

								<ul>
									<li>Higher ROA</li>
									<li>Higher short-term stock returns</li>
									<li>greater future operational performance</li>
								</ul>
							</li>
						

							<li>Negative tone in media coverage has been linked to:

								<ul>
									<li>lower stock returns</li>
									<li>lower earnings</li>
									<li>higher stock market volatility</li>
									<li>higher stock market volatility</li>
								</ul>
							</li>

						</ul>
					</section>


					<section>
						<p>Discussion</p>
						<!-- <p>Made with <a href="https://danielabaron.me/blog/build-and-publish-presentation-with-html-and-css/">reveal.js</a></p>  -->
					</section>
				</section>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/menu1/menu.js"></script>
		
		<script>
			Reveal.initialize({
				hash: true,
				menu: {
					titleSelector: 'h5',
					useTextContentForMissingTitles: true,
					markers: false,
				},

				customcontrols: {
					controls: [
					{ icon: '<i class="fa fa-pen-square"></i>',
						title: 'Toggle chalkboard (B)',
						action: 'RevealChalkboard.toggleChalkboard();'
					},
					{ icon: '<i class="fa fa-pen"></i>',
						title: 'Toggle notes canvas (C)',
						action: 'RevealChalkboard.toggleNotesCanvas();'
					}
					]
				},
				chalkboard: {
					// add configuration here
				},
				// ...
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealChalkboard, RevealCustomControls, RevealMenu ],
				// ...
				});
		</script>
		
		
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			// Reveal.initialize({
			// 	hash: true,

			// 	// Learn about plugins: https://revealjs.com/plugins/
			// 	plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			// });
		</script>
	</body>
</html>
